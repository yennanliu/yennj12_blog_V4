---
title: "Kubernetes å®Œæ•´æŒ‡å—ï¼ˆä¸‰ï¼‰ï¼šé€²éšåŠŸèƒ½èˆ‡ç”Ÿç”¢ç’°å¢ƒå¯¦è¸"
date: 2025-10-11T13:00:00+08:00
draft: false
description: "æ·±å…¥æ¢è¨ Kubernetes é€²éšä¸»é¡Œï¼ŒåŒ…å«è‡ªå‹•æ“´å±•ã€RBAC æ¬Šé™ç®¡ç†ã€Network Policyã€Helm å¥—ä»¶ç®¡ç†ã€ç›£æ§å‘Šè­¦ã€æ—¥èªŒæ”¶é›†ã€CI/CD æ•´åˆèˆ‡ç”Ÿç”¢ç’°å¢ƒæœ€ä½³å¯¦è¸ï¼Œæ‰“é€ ä¼æ¥­ç´š K8S å¹³å°ã€‚"
categories: ["Engineering", "DevOps", "Kubernetes", "all"]
tags: ["Kubernetes", "K8S", "HPA", "RBAC", "Helm", "ç›£æ§", "Prometheus", "Grafana", "CI/CD", "ç”Ÿç”¢éƒ¨ç½²", "æœ€ä½³å¯¦è¸"]
authors: ["yennj12 team"]
readTime: "75 min"
---

## ğŸ¯ å‰è¨€

ç¶“éå‰å…©ç¯‡çš„å­¸ç¿’ï¼Œæˆ‘å€‘å·²ç¶“æŒæ¡äº† Kubernetes çš„åŸºç¤æ¦‚å¿µèˆ‡æ ¸å¿ƒè³‡æºæ“ä½œã€‚æœ¬æ–‡å°‡æ·±å…¥æ¢è¨é€²éšåŠŸèƒ½èˆ‡ç”Ÿç”¢ç’°å¢ƒå¯¦è¸ï¼Œå¹«åŠ©ä½ æ§‹å»ºä¼æ¥­ç´šçš„å®¹å™¨å¹³å°ã€‚

**æœ¬æ–‡é‡é»ï¼š**
- è‡ªå‹•æ“´å±•ï¼ˆHPA/VPA/CAï¼‰
- RBAC æ¬Šé™ç®¡ç†
- Network Policy ç¶²è·¯ç­–ç•¥
- Helm å¥—ä»¶ç®¡ç†
- ç›£æ§èˆ‡å‘Šè­¦ç³»çµ±
- æ—¥èªŒæ”¶é›†æ–¹æ¡ˆ
- CI/CD æ•´åˆ
- ç”Ÿç”¢ç’°å¢ƒæœ€ä½³å¯¦è¸

## âš¡ è‡ªå‹•æ“´å±•æ©Ÿåˆ¶

### æ“´å±•é¡å‹å°ç…§

```mermaid
graph TB
    A[Kubernetes è‡ªå‹•æ“´å±•] --> B[HPA<br/>æ°´å¹³ Pod æ“´å±•]
    A --> C[VPA<br/>å‚ç›´ Pod æ“´å±•]
    A --> D[CA<br/>å¢é›†è‡ªå‹•æ“´å±•]

    B --> B1[æ ¹æ“š CPU/è¨˜æ†¶é«”<br/>è‡ªå‹•èª¿æ•´ Pod æ•¸é‡]
    C --> C1[æ ¹æ“šè³‡æºä½¿ç”¨<br/>èª¿æ•´ Pod è³‡æºé™åˆ¶]
    D --> D1[æ ¹æ“šè² è¼‰<br/>è‡ªå‹•å¢æ¸›ç¯€é»]

    style A fill:#326ce5
    style B fill:#4ecdc4
    style C fill:#feca57
    style D fill:#ff6b6b
```

### HPA (Horizontal Pod Autoscaler)

**åŸºæ–¼ CPU çš„ HPAï¼š**

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: nginx-hpa
  namespace: default
spec:
  # ç›®æ¨™ Deployment
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx

  # Pod æ•¸é‡ç¯„åœ
  minReplicas: 2
  maxReplicas: 10

  # æ“´å±•è¡Œç‚º
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300  # ç¸®å®¹ç©©å®šè¦–çª—
      policies:
      - type: Percent
        value: 50  # æ¯æ¬¡æœ€å¤šç¸®å®¹ 50%
        periodSeconds: 60
      - type: Pods
        value: 2   # æ¯æ¬¡æœ€å¤šç¸®å®¹ 2 å€‹ Pod
        periodSeconds: 60
      selectPolicy: Min  # é¸æ“‡æœ€å°å€¼

    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 100  # æ¯æ¬¡æœ€å¤šæ“´å®¹ 100%
        periodSeconds: 30
      - type: Pods
        value: 4    # æ¯æ¬¡æœ€å¤šæ“´å®¹ 4 å€‹ Pod
        periodSeconds: 30
      selectPolicy: Max  # é¸æ“‡æœ€å¤§å€¼

  # æŒ‡æ¨™é…ç½®
  metrics:
  # CPU ä½¿ç”¨ç‡
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70  # ç›®æ¨™ CPU ä½¿ç”¨ç‡ 70%

  # è¨˜æ†¶é«”ä½¿ç”¨ç‡
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80  # ç›®æ¨™è¨˜æ†¶é«”ä½¿ç”¨ç‡ 80%

  # è‡ªè¨‚æŒ‡æ¨™ï¼ˆPrometheusï¼‰
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "1000"

  # å¤–éƒ¨æŒ‡æ¨™
  - type: External
    external:
      metric:
        name: queue_length
        selector:
          matchLabels:
            queue: worker_tasks
      target:
        type: AverageValue
        averageValue: "30"
```

### HPA æ“ä½œæŒ‡ä»¤

```bash
# å‰µå»º HPAï¼ˆç°¡å–®ç‰ˆï¼‰
kubectl autoscale deployment nginx --min=2 --max=10 --cpu-percent=70

# å‰µå»º HPAï¼ˆYAMLï¼‰
kubectl apply -f hpa.yaml

# æŸ¥çœ‹ HPA
kubectl get hpa
kubectl describe hpa nginx-hpa

# ç›£è¦– HPA
kubectl get hpa --watch

# æ‰‹å‹•æ¸¬è©¦ï¼ˆç”¢ç”Ÿè² è¼‰ï¼‰
kubectl run -it --rm load-generator --image=busybox -- /bin/sh
while true; do wget -q -O- http://nginx-service; done

# åˆªé™¤ HPA
kubectl delete hpa nginx-hpa
```

### VPA (Vertical Pod Autoscaler)

```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: nginx-vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx

  # æ›´æ–°ç­–ç•¥
  updatePolicy:
    updateMode: "Auto"  # Auto, Recreate, Initial, Off

  # è³‡æºç­–ç•¥
  resourcePolicy:
    containerPolicies:
    - containerName: nginx
      minAllowed:
        cpu: 100m
        memory: 50Mi
      maxAllowed:
        cpu: 2
        memory: 1Gi
      mode: Auto
```

**VPA æ¨¡å¼å°ç…§è¡¨ï¼š**

| æ¨¡å¼ | èªªæ˜ | è¡Œç‚º |
|------|------|------|
| **Off** | åƒ…æä¾›å»ºè­° | ä¸è‡ªå‹•èª¿æ•´ |
| **Initial** | å‰µå»ºæ™‚è¨­å®š | åªåœ¨å‰µå»ºæ™‚æ‡‰ç”¨ |
| **Recreate** | é‡å»º Pod | åˆªé™¤ä¸¦é‡å»º Pod |
| **Auto** | è‡ªå‹•èª¿æ•´ | å°±åœ°æ›´æ–°æˆ–é‡å»º |

### Cluster Autoscaler

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cluster-autoscaler
  template:
    metadata:
      labels:
        app: cluster-autoscaler
    spec:
      serviceAccountName: cluster-autoscaler
      containers:
      - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.27.0
        name: cluster-autoscaler
        command:
        - ./cluster-autoscaler
        - --v=4
        - --stderrthreshold=info
        - --cloud-provider=aws
        - --skip-nodes-with-local-storage=false
        - --expander=least-waste
        - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/my-cluster
        - --balance-similar-node-groups
        - --skip-nodes-with-system-pods=false
        env:
        - name: AWS_REGION
          value: us-west-2
```

## ğŸ” RBAC æ¬Šé™ç®¡ç†

### RBAC æ¶æ§‹

```mermaid
graph TB
    subgraph "ä¸»é«” (Subject)"
        U[User<br/>ä½¿ç”¨è€…]
        G[Group<br/>ç¾¤çµ„]
        SA[ServiceAccount<br/>æœå‹™å¸³è™Ÿ]
    end

    subgraph "ç¹«çµ (Binding)"
        RB[RoleBinding<br/>å‘½åç©ºé–“ç´šåˆ¥]
        CRB[ClusterRoleBinding<br/>å¢é›†ç´šåˆ¥]
    end

    subgraph "è§’è‰² (Role)"
        R[Role<br/>å‘½åç©ºé–“ç´šåˆ¥]
        CR[ClusterRole<br/>å¢é›†ç´šåˆ¥]
    end

    subgraph "è³‡æº (Resources)"
        P[Pods]
        D[Deployments]
        S[Services]
        N[Nodes]
    end

    U --> RB
    G --> RB
    SA --> RB

    U --> CRB
    G --> CRB
    SA --> CRB

    RB --> R
    CRB --> CR

    R -.->|å­˜å–| P
    R -.->|å­˜å–| D
    R -.->|å­˜å–| S

    CR -.->|å­˜å–| N
    CR -.->|å­˜å–| P
    CR -.->|å­˜å–| D

    style U fill:#326ce5
    style RB fill:#4ecdc4
    style R fill:#feca57
    style P fill:#ff6b6b
```

### Role èˆ‡ ClusterRole

**Roleï¼ˆå‘½åç©ºé–“ç´šåˆ¥ï¼‰ï¼š**

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: pod-reader
rules:
# Pod è³‡æº
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "watch", "list"]

# Pod æ—¥èªŒ
- apiGroups: [""]
  resources: ["pods/log"]
  verbs: ["get", "list"]

# ConfigMap èˆ‡ Secret
- apiGroups: [""]
  resources: ["configmaps", "secrets"]
  verbs: ["get"]

# Deployment
- apiGroups: ["apps"]
  resources: ["deployments"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]

# Service
- apiGroups: [""]
  resources: ["services"]
  verbs: ["get", "list", "create", "delete"]
```

**ClusterRoleï¼ˆå¢é›†ç´šåˆ¥ï¼‰ï¼š**

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cluster-admin-custom
rules:
# æ‰€æœ‰è³‡æºçš„å®Œæ•´æ¬Šé™
- apiGroups: ["*"]
  resources: ["*"]
  verbs: ["*"]

# éè³‡æº URL
- nonResourceURLs: ["*"]
  verbs: ["*"]
```

### RoleBinding èˆ‡ ClusterRoleBinding

**RoleBindingï¼š**

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods
  namespace: default
subjects:
# ä½¿ç”¨è€…
- kind: User
  name: jane
  apiGroup: rbac.authorization.k8s.io

# ç¾¤çµ„
- kind: Group
  name: developers
  apiGroup: rbac.authorization.k8s.io

# ServiceAccount
- kind: ServiceAccount
  name: my-service-account
  namespace: default

roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
```

**ClusterRoleBindingï¼š**

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: read-all-pods
subjects:
- kind: Group
  name: system:authenticated
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: view
  apiGroup: rbac.authorization.k8s.io
```

### ServiceAccount

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-app-sa
  namespace: default
automountServiceAccountToken: true
secrets:
- name: my-app-token

---
apiVersion: v1
kind: Secret
metadata:
  name: my-app-token
  namespace: default
  annotations:
    kubernetes.io/service-account.name: my-app-sa
type: kubernetes.io/service-account-token
```

### RBAC æ“ä½œæŒ‡ä»¤

```bash
# æŸ¥çœ‹è§’è‰²
kubectl get roles
kubectl get clusterroles
kubectl describe role pod-reader

# æŸ¥çœ‹ç¹«çµ
kubectl get rolebindings
kubectl get clusterrolebindings
kubectl describe rolebinding read-pods

# æŸ¥çœ‹ ServiceAccount
kubectl get serviceaccounts
kubectl get sa  # ç°¡å¯«
kubectl describe sa my-app-sa

# æª¢æŸ¥æ¬Šé™
kubectl auth can-i create deployments
kubectl auth can-i delete pods --namespace=default
kubectl auth can-i '*' '*' --all-namespaces

# ä»¥ç‰¹å®šä½¿ç”¨è€…èº«åˆ†æª¢æŸ¥
kubectl auth can-i list pods --as=jane
kubectl auth can-i create deployments --as=system:serviceaccount:default:my-app-sa

# å‰µå»º ServiceAccount Token
kubectl create token my-app-sa --duration=24h

# æŸ¥çœ‹ç•¶å‰ä½¿ç”¨è€…
kubectl config view --minify -o jsonpath='{.contexts[0].context.user}'
```

### é è¨­ ClusterRole

| ClusterRole | èªªæ˜ | æ¬Šé™ç¯„åœ |
|-------------|------|----------|
| **cluster-admin** | è¶…ç´šç®¡ç†å“¡ | å®Œæ•´æ¬Šé™ |
| **admin** | å‘½åç©ºé–“ç®¡ç†å“¡ | å‘½åç©ºé–“å…§å®Œæ•´æ¬Šé™ |
| **edit** | ç·¨è¼¯è€… | è®€å¯«å¤§éƒ¨åˆ†è³‡æº |
| **view** | æª¢è¦–è€… | å”¯è®€æ¬Šé™ |

## ğŸŒ Network Policy

### Network Policy æ¦‚å¿µ

```mermaid
graph TB
    subgraph "Frontend Namespace"
        WEB[Web Pod]
    end

    subgraph "Backend Namespace"
        API[API Pod]
        CACHE[Cache Pod]
    end

    subgraph "Database Namespace"
        DB[(Database Pod)]
    end

    WEB -->|å…è¨±| API
    API -->|å…è¨±| DB
    API -->|å…è¨±| CACHE
    WEB -.->|æ‹’çµ•| DB
    WEB -.->|æ‹’çµ•| CACHE

    INTERNET[ç¶²éš›ç¶²è·¯] -->|å…è¨±| WEB
    INTERNET -.->|æ‹’çµ•| API
    INTERNET -.->|æ‹’çµ•| DB

    style WEB fill:#4ecdc4
    style API fill:#feca57
    style DB fill:#ff6b6b
```

### Network Policy å®Œæ•´ç¯„ä¾‹

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: api-network-policy
  namespace: backend
spec:
  # æ‡‰ç”¨åˆ°å“ªäº› Pod
  podSelector:
    matchLabels:
      app: api
      tier: backend

  # ç­–ç•¥é¡å‹
  policyTypes:
  - Ingress  # å…¥ç«™æµé‡
  - Egress   # å‡ºç«™æµé‡

  # å…¥ç«™è¦å‰‡
  ingress:
  # è¦å‰‡ 1: å…è¨±ä¾†è‡ª frontend çš„æµé‡
  - from:
    - namespaceSelector:
        matchLabels:
          name: frontend
      podSelector:
        matchLabels:
          app: web
    ports:
    - protocol: TCP
      port: 8080

  # è¦å‰‡ 2: å…è¨±ä¾†è‡ªç‰¹å®š IP ç¯„åœ
  - from:
    - ipBlock:
        cidr: 10.0.0.0/16
        except:
        - 10.0.1.0/24
    ports:
    - protocol: TCP
      port: 8080

  # å‡ºç«™è¦å‰‡
  egress:
  # è¦å‰‡ 1: å…è¨±å­˜å–è³‡æ–™åº«
  - to:
    - namespaceSelector:
        matchLabels:
          name: database
      podSelector:
        matchLabels:
          app: postgres
    ports:
    - protocol: TCP
      port: 5432

  # è¦å‰‡ 2: å…è¨±å­˜å– Redis
  - to:
    - podSelector:
        matchLabels:
          app: redis
    ports:
    - protocol: TCP
      port: 6379

  # è¦å‰‡ 3: å…è¨± DNS æŸ¥è©¢
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
      podSelector:
        matchLabels:
          k8s-app: kube-dns
    ports:
    - protocol: UDP
      port: 53

  # è¦å‰‡ 4: å…è¨±å­˜å–å¤–éƒ¨ API
  - to:
    - ipBlock:
        cidr: 0.0.0.0/0
        except:
        - 10.0.0.0/8
        - 172.16.0.0/12
        - 192.168.0.0/16
    ports:
    - protocol: TCP
      port: 443
```

### Network Policy å¸¸è¦‹æ¨¡å¼

**1. é è¨­æ‹’çµ•æ‰€æœ‰æµé‡ï¼š**

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
  namespace: production
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
```

**2. å…è¨±ç‰¹å®šå‘½åç©ºé–“ï¼š**

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-namespace
spec:
  podSelector:
    matchLabels:
      app: backend
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          environment: production
```

**3. å…è¨± DNSï¼š**

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-dns
spec:
  podSelector: {}
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    - podSelector:
        matchLabels:
          k8s-app: kube-dns
    ports:
    - protocol: UDP
      port: 53
```

### Network Policy æ“ä½œæŒ‡ä»¤

```bash
# å‰µå»º Network Policy
kubectl apply -f network-policy.yaml

# æŸ¥çœ‹ Network Policy
kubectl get networkpolicies
kubectl get netpol  # ç°¡å¯«
kubectl describe networkpolicy api-network-policy

# æ¸¬è©¦ç¶²è·¯é€£é€šæ€§ï¼ˆå¾ Pod A æ¸¬è©¦é€£åˆ° Pod Bï¼‰
kubectl exec -it pod-a -- curl http://pod-b-service

# åˆªé™¤ Network Policy
kubectl delete networkpolicy api-network-policy
```

## ğŸ“¦ Helm å¥—ä»¶ç®¡ç†

### Helm æ¶æ§‹

```mermaid
graph TB
    H[Helm CLI] --> CHART[Chart<br/>å¥—ä»¶å®šç¾©]
    CHART --> TEMPLATE[Templates<br/>YAML æ¨¡æ¿]
    CHART --> VALUES[values.yaml<br/>é…ç½®å€¼]
    CHART --> CHART_YAML[Chart.yaml<br/>å…ƒè³‡æ–™]

    H -->|helm install| K8S[Kubernetes API]
    K8S --> RELEASE[Release<br/>éƒ¨ç½²å¯¦ä¾‹]

    REPO[Helm Repository] -.->|helm pull| CHART

    style H fill:#326ce5
    style CHART fill:#4ecdc4
    style K8S fill:#feca57
    style RELEASE fill:#ff6b6b
```

### Helm Chart çµæ§‹

```
my-app/
â”œâ”€â”€ Chart.yaml          # Chart å…ƒè³‡æ–™
â”œâ”€â”€ values.yaml         # é è¨­å€¼
â”œâ”€â”€ values-dev.yaml     # é–‹ç™¼ç’°å¢ƒå€¼
â”œâ”€â”€ values-prod.yaml    # ç”Ÿç”¢ç’°å¢ƒå€¼
â”œâ”€â”€ templates/          # K8s è³‡æºæ¨¡æ¿
â”‚   â”œâ”€â”€ deployment.yaml
â”‚   â”œâ”€â”€ service.yaml
â”‚   â”œâ”€â”€ ingress.yaml
â”‚   â”œâ”€â”€ configmap.yaml
â”‚   â”œâ”€â”€ secret.yaml
â”‚   â”œâ”€â”€ hpa.yaml
â”‚   â”œâ”€â”€ _helpers.tpl   # è¼”åŠ©å‡½æ•¸
â”‚   â”œâ”€â”€ NOTES.txt      # å®‰è£èªªæ˜
â”‚   â””â”€â”€ tests/         # æ¸¬è©¦
â”‚       â””â”€â”€ test-connection.yaml
â”œâ”€â”€ charts/            # ä¾è³´ Chart
â”œâ”€â”€ .helmignore        # å¿½ç•¥æª”æ¡ˆ
â””â”€â”€ README.md
```

### Chart.yaml ç¯„ä¾‹

```yaml
apiVersion: v2
name: my-app
description: My Application Helm Chart
type: application
version: 1.0.0
appVersion: "1.24.0"
keywords:
  - web
  - application
home: https://example.com
sources:
  - https://github.com/example/my-app
maintainers:
  - name: DevOps Team
    email: devops@example.com
dependencies:
  - name: postgresql
    version: "12.1.0"
    repository: "https://charts.bitnami.com/bitnami"
    condition: postgresql.enabled
  - name: redis
    version: "17.0.0"
    repository: "https://charts.bitnami.com/bitnami"
    condition: redis.enabled
```

### values.yaml ç¯„ä¾‹

```yaml
# å‰¯æœ¬æ•¸
replicaCount: 3

# æ˜ åƒé…ç½®
image:
  repository: myapp
  pullPolicy: IfNotPresent
  tag: "1.24.0"

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

# ServiceAccount
serviceAccount:
  create: true
  annotations: {}
  name: ""

# Pod è¨»è§£
podAnnotations:
  prometheus.io/scrape: "true"
  prometheus.io/port: "9090"

# Pod å®‰å…¨ä¸Šä¸‹æ–‡
podSecurityContext:
  runAsNonRoot: true
  runAsUser: 1000
  fsGroup: 2000

securityContext:
  capabilities:
    drop:
    - ALL
  readOnlyRootFilesystem: true
  allowPrivilegeEscalation: false

# Service é…ç½®
service:
  type: ClusterIP
  port: 80
  targetPort: 8080

# Ingress é…ç½®
ingress:
  enabled: true
  className: nginx
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
  hosts:
    - host: app.example.com
      paths:
        - path: /
          pathType: Prefix
  tls:
    - secretName: app-tls
      hosts:
        - app.example.com

# è³‡æºé™åˆ¶
resources:
  limits:
    cpu: 500m
    memory: 512Mi
  requests:
    cpu: 250m
    memory: 256Mi

# HPA
autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 10
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80

# NodeSelector
nodeSelector: {}

# Tolerations
tolerations: []

# Affinity
affinity: {}

# ç’°å¢ƒè®Šæ•¸
env:
  - name: ENVIRONMENT
    value: "production"
  - name: LOG_LEVEL
    value: "info"

# ConfigMap
configMap:
  data:
    app.properties: |
      server.port=8080
      server.host=0.0.0.0

# Secret
secret:
  data:
    database-password: ""
    api-key: ""

# PostgreSQL ä¾è³´
postgresql:
  enabled: true
  auth:
    username: myapp
    password: ""
    database: myapp
  primary:
    persistence:
      enabled: true
      size: 10Gi

# Redis ä¾è³´
redis:
  enabled: true
  auth:
    enabled: true
    password: ""
  master:
    persistence:
      enabled: true
      size: 8Gi
```

### templates/deployment.yaml ç¯„ä¾‹

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "my-app.fullname" . }}
  labels:
    {{- include "my-app.labels" . | nindent 4 }}
spec:
  {{- if not .Values.autoscaling.enabled }}
  replicas: {{ .Values.replicaCount }}
  {{- end }}
  selector:
    matchLabels:
      {{- include "my-app.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      annotations:
        checksum/config: {{ include (print $.Template.BasePath "/configmap.yaml") . | sha256sum }}
        {{- with .Values.podAnnotations }}
        {{- toYaml . | nindent 8 }}
        {{- end }}
      labels:
        {{- include "my-app.selectorLabels" . | nindent 8 }}
    spec:
      {{- with .Values.imagePullSecrets }}
      imagePullSecrets:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      serviceAccountName: {{ include "my-app.serviceAccountName" . }}
      securityContext:
        {{- toYaml .Values.podSecurityContext | nindent 8 }}
      containers:
      - name: {{ .Chart.Name }}
        securityContext:
          {{- toYaml .Values.securityContext | nindent 12 }}
        image: "{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"
        imagePullPolicy: {{ .Values.image.pullPolicy }}
        ports:
        - name: http
          containerPort: {{ .Values.service.targetPort }}
          protocol: TCP
        livenessProbe:
          httpGet:
            path: /health
            port: http
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: http
          initialDelaySeconds: 5
          periodSeconds: 5
        env:
          {{- toYaml .Values.env | nindent 12 }}
        resources:
          {{- toYaml .Values.resources | nindent 12 }}
      {{- with .Values.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
```

### Helm æ“ä½œæŒ‡ä»¤

```bash
# å®‰è£ Helm
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

# æ·»åŠ å€‰åº«
helm repo add stable https://charts.helm.sh/stable
helm repo add bitnami https://charts.bitnami.com/bitnami
helm repo update

# æœå°‹ Chart
helm search repo nginx
helm search hub wordpress

# æŸ¥çœ‹ Chart è³‡è¨Š
helm show chart bitnami/nginx
helm show values bitnami/nginx
helm show readme bitnami/nginx

# å®‰è£ Chart
helm install my-release bitnami/nginx
helm install my-app ./my-app-chart
helm install my-app ./my-app-chart -f values-prod.yaml
helm install my-app ./my-app-chart --set replicaCount=5
helm install my-app ./my-app-chart --namespace production --create-namespace

# æŸ¥çœ‹ Release
helm list
helm list -A  # æ‰€æœ‰å‘½åç©ºé–“
helm status my-app
helm get values my-app
helm get manifest my-app

# å‡ç´š Release
helm upgrade my-app ./my-app-chart
helm upgrade my-app ./my-app-chart -f values-prod.yaml
helm upgrade my-app ./my-app-chart --set image.tag=1.25.0
helm upgrade --install my-app ./my-app-chart  # ä¸å­˜åœ¨å‰‡å®‰è£

# å›æ»¾ Release
helm rollback my-app
helm rollback my-app 2  # å›æ»¾åˆ°ç‰ˆæœ¬ 2
helm history my-app

# åˆªé™¤ Release
helm uninstall my-app
helm uninstall my-app --keep-history

# é©—è­‰ Chart
helm lint ./my-app-chart
helm template my-app ./my-app-chart
helm install --dry-run --debug my-app ./my-app-chart

# æ‰“åŒ… Chart
helm package ./my-app-chart
helm package ./my-app-chart --version 1.0.1

# å‰µå»º Chart
helm create my-new-chart

# ä¾è³´ç®¡ç†
helm dependency update ./my-app-chart
helm dependency build ./my-app-chart
helm dependency list ./my-app-chart
```

## ğŸ“Š ç›£æ§èˆ‡å‘Šè­¦ç³»çµ±

### Prometheus + Grafana æ¶æ§‹

```mermaid
graph TB
    subgraph "è³‡æ–™æ”¶é›†"
        NE[Node Exporter<br/>ç¯€é»æŒ‡æ¨™]
        KSM[Kube State Metrics<br/>K8s è³‡æºç‹€æ…‹]
        CA[cAdvisor<br/>å®¹å™¨æŒ‡æ¨™]
        APP[Application<br/>è‡ªè¨‚æŒ‡æ¨™]
    end

    subgraph "Prometheus"
        PROM[Prometheus Server<br/>æ™‚åºè³‡æ–™åº«]
        ALERT[Alertmanager<br/>å‘Šè­¦ç®¡ç†]
    end

    subgraph "è¦–è¦ºåŒ–"
        GRAF[Grafana<br/>å„€è¡¨æ¿]
    end

    NE -->|metrics| PROM
    KSM -->|metrics| PROM
    CA -->|metrics| PROM
    APP -->|metrics| PROM

    PROM -->|alerts| ALERT
    ALERT -->|é€šçŸ¥| EMAIL[Email]
    ALERT -->|é€šçŸ¥| SLACK[Slack]
    ALERT -->|é€šçŸ¥| WEBHOOK[Webhook]

    GRAF -->|æŸ¥è©¢| PROM

    style PROM fill:#326ce5
    style GRAF fill:#ff6b6b
    style ALERT fill:#feca57
```

### Prometheus å®‰è£ï¼ˆHelmï¼‰

```bash
# æ·»åŠ  Prometheus ç¤¾ç¾¤ Helm å€‰åº«
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

# å®‰è£ kube-prometheus-stack
helm install prometheus prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --create-namespace \
  --set prometheus.prometheusSpec.retention=30d \
  --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=50Gi \
  --set grafana.adminPassword=admin123

# æŸ¥çœ‹å®‰è£çš„è³‡æº
kubectl get all -n monitoring

# å­˜å– Prometheus
kubectl port-forward -n monitoring svc/prometheus-kube-prometheus-prometheus 9090:9090

# å­˜å– Grafana
kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80
```

### ServiceMonitor é…ç½®

```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: my-app-metrics
  namespace: monitoring
  labels:
    release: prometheus
spec:
  selector:
    matchLabels:
      app: my-app
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics
  namespaceSelector:
    matchNames:
    - default
```

### PrometheusRule å‘Šè­¦è¦å‰‡

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: my-app-alerts
  namespace: monitoring
  labels:
    release: prometheus
spec:
  groups:
  - name: my-app
    interval: 30s
    rules:
    # Pod é‡å•Ÿéå¤š
    - alert: PodRestarting
      expr: |
        rate(kube_pod_container_status_restarts_total[15m]) > 0
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} restarting"
        description: "Pod has restarted {{ $value }} times in the last 15 minutes"

    # CPU ä½¿ç”¨ç‡éé«˜
    - alert: HighCPUUsage
      expr: |
        sum(rate(container_cpu_usage_seconds_total{namespace="default"}[5m])) by (pod) > 0.8
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "High CPU usage detected"
        description: "Pod {{ $labels.pod }} CPU usage is {{ $value | humanizePercentage }}"

    # è¨˜æ†¶é«”ä½¿ç”¨ç‡éé«˜
    - alert: HighMemoryUsage
      expr: |
        sum(container_memory_working_set_bytes{namespace="default"}) by (pod) /
        sum(container_spec_memory_limit_bytes{namespace="default"}) by (pod) > 0.9
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "High memory usage detected"
        description: "Pod {{ $labels.pod }} memory usage is {{ $value | humanizePercentage }}"

    # æœå‹™ä¸å¯ç”¨
    - alert: ServiceDown
      expr: |
        up{job="my-app"} == 0
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "Service is down"
        description: "The service {{ $labels.job }} has been down for more than 1 minute"

    # HTTP éŒ¯èª¤ç‡éé«˜
    - alert: HighErrorRate
      expr: |
        sum(rate(http_requests_total{status=~"5.."}[5m])) by (service) /
        sum(rate(http_requests_total[5m])) by (service) > 0.05
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "High HTTP error rate"
        description: "Service {{ $labels.service }} error rate is {{ $value | humanizePercentage }}"
```

## ğŸ“ æ—¥èªŒæ”¶é›†æ–¹æ¡ˆ

### EFK Stack æ¶æ§‹

```mermaid
graph TB
    subgraph "æ—¥èªŒä¾†æº"
        POD1[Pod 1]
        POD2[Pod 2]
        POD3[Pod 3]
        NODE[Node Logs]
    end

    subgraph "æ—¥èªŒæ”¶é›†"
        FB[Fluent Bit<br/>DaemonSet]
    end

    subgraph "æ—¥èªŒè™•ç†"
        ES[Elasticsearch<br/>å„²å­˜èˆ‡ç´¢å¼•]
    end

    subgraph "æ—¥èªŒè¦–è¦ºåŒ–"
        KB[Kibana<br/>æŸ¥è©¢èˆ‡åˆ†æ]
    end

    POD1 -->|stdout/stderr| FB
    POD2 -->|stdout/stderr| FB
    POD3 -->|stdout/stderr| FB
    NODE -->|/var/log| FB

    FB -->|è½‰ç™¼| ES
    KB -->|æŸ¥è©¢| ES

    style FB fill:#326ce5
    style ES fill:#4ecdc4
    style KB fill:#ff6b6b
```

### Fluent Bit é…ç½®

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluent-bit-config
  namespace: logging
data:
  fluent-bit.conf: |
    [SERVICE]
        Daemon Off
        Flush 1
        Log_Level info
        Parsers_File parsers.conf
        HTTP_Server On
        HTTP_Listen 0.0.0.0
        HTTP_Port 2020
        Health_Check On

    [INPUT]
        Name              tail
        Path              /var/log/containers/*.log
        multiline.parser  docker, cri
        Tag               kube.*
        Mem_Buf_Limit     5MB
        Skip_Long_Lines   On

    [FILTER]
        Name                kubernetes
        Match               kube.*
        Kube_URL            https://kubernetes.default.svc:443
        Kube_CA_File        /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        Kube_Token_File     /var/run/secrets/kubernetes.io/serviceaccount/token
        Kube_Tag_Prefix     kube.var.log.containers.
        Merge_Log           On
        Keep_Log            Off
        K8S-Logging.Parser  On
        K8S-Logging.Exclude On

    [OUTPUT]
        Name            es
        Match           *
        Host            elasticsearch.logging.svc.cluster.local
        Port            9200
        Logstash_Format On
        Retry_Limit     False
        Type            _doc

  parsers.conf: |
    [PARSER]
        Name   json
        Format json
        Time_Key time
        Time_Format %d/%b/%Y:%H:%M:%S %z

    [PARSER]
        Name        docker
        Format      json
        Time_Key    time
        Time_Format %Y-%m-%dT%H:%M:%S.%L
        Time_Keep   On

    [PARSER]
        Name        syslog
        Format      regex
        Regex       ^\<(?<pri>[0-9]+)\>(?<time>[^ ]* {1,2}[^ ]* [^ ]*) (?<host>[^ ]*) (?<ident>[a-zA-Z0-9_\/\.\-]*)(?:\[(?<pid>[0-9]+)\])?(?:[^\:]*\:)? *(?<message>.*)$
        Time_Key    time
        Time_Format %b %d %H:%M:%S
```

## ğŸš€ CI/CD æ•´åˆ

### GitLab CI/CD Pipeline

```yaml
# .gitlab-ci.yml
variables:
  DOCKER_REGISTRY: registry.example.com
  IMAGE_NAME: ${DOCKER_REGISTRY}/myapp
  KUBE_NAMESPACE: production
  KUBECONFIG: /etc/deploy/config

stages:
  - test
  - build
  - deploy

# æ¸¬è©¦éšæ®µ
test:
  stage: test
  image: node:18
  script:
    - npm ci
    - npm run lint
    - npm run test
    - npm run test:coverage
  coverage: '/Statements\s+:\s+(\d+\.\d+)%/'
  artifacts:
    reports:
      coverage_report:
        coverage_format: cobertura
        path: coverage/cobertura-coverage.xml
    paths:
      - coverage/
  only:
    - branches

# å»ºç«‹æ˜ åƒ
build:
  stage: build
  image: docker:24
  services:
    - docker:24-dind
  before_script:
    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $DOCKER_REGISTRY
  script:
    - docker build
        --build-arg VERSION=${CI_COMMIT_SHORT_SHA}
        -t ${IMAGE_NAME}:${CI_COMMIT_SHORT_SHA}
        -t ${IMAGE_NAME}:latest
        .
    - docker push ${IMAGE_NAME}:${CI_COMMIT_SHORT_SHA}
    - docker push ${IMAGE_NAME}:latest
  only:
    - main
    - develop

# éƒ¨ç½²åˆ°é–‹ç™¼ç’°å¢ƒ
deploy:dev:
  stage: deploy
  image: bitnami/kubectl:latest
  script:
    - kubectl config use-context dev-cluster
    - kubectl set image deployment/myapp myapp=${IMAGE_NAME}:${CI_COMMIT_SHORT_SHA} -n development
    - kubectl rollout status deployment/myapp -n development
  environment:
    name: development
    url: https://dev.example.com
  only:
    - develop

# éƒ¨ç½²åˆ°ç”Ÿç”¢ç’°å¢ƒ
deploy:prod:
  stage: deploy
  image: bitnami/kubectl:latest
  script:
    - kubectl config use-context prod-cluster
    - |
      helm upgrade --install myapp ./helm/myapp \
        --namespace ${KUBE_NAMESPACE} \
        --set image.tag=${CI_COMMIT_SHORT_SHA} \
        --set replicaCount=3 \
        --values ./helm/myapp/values-prod.yaml \
        --wait \
        --timeout 5m
    - kubectl get pods -n ${KUBE_NAMESPACE} -l app=myapp
  environment:
    name: production
    url: https://example.com
  when: manual
  only:
    - main
```

### GitHub Actions Workflow

```yaml
# .github/workflows/deploy.yml
name: Build and Deploy to Kubernetes

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '18'
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Run tests
        run: |
          npm run lint
          npm run test
          npm run test:coverage

      - name: Upload coverage
        uses: codecov/codecov-action@v3
        with:
          files: ./coverage/lcov.info

  build:
    needs: test
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write
    steps:
      - uses: actions/checkout@v3

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2

      - name: Log in to Container Registry
        uses: docker/login-action@v2
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v4
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            type=sha,prefix={{branch}}-

      - name: Build and push
        uses: docker/build-push-action@v4
        with:
          context: .
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

  deploy:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    steps:
      - uses: actions/checkout@v3

      - name: Setup kubectl
        uses: azure/setup-kubectl@v3

      - name: Configure kubectl
        run: |
          echo "${{ secrets.KUBECONFIG }}" | base64 -d > kubeconfig
          export KUBECONFIG=kubeconfig

      - name: Deploy to Kubernetes
        run: |
          kubectl set image deployment/myapp \
            myapp=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:main-${{ github.sha }} \
            -n production

          kubectl rollout status deployment/myapp -n production

      - name: Verify deployment
        run: |
          kubectl get pods -n production -l app=myapp
          kubectl get svc -n production -l app=myapp
```

## ğŸ¯ ç”Ÿç”¢ç’°å¢ƒæœ€ä½³å¯¦è¸

### å®‰å…¨æ€§æœ€ä½³å¯¦è¸æ¸…å–®

| é¡åˆ¥ | æœ€ä½³å¯¦è¸ | å¯¦æ–½æ–¹æ³• |
|------|----------|----------|
| **æ˜ åƒå®‰å…¨** | ä½¿ç”¨æœ€å°åŒ–åŸºç¤æ˜ åƒ | Alpine, Distroless |
| | å®šæœŸæƒææ¼æ´ | Trivy, Clair |
| | ä½¿ç”¨ç§æœ‰ Registry | Harbor, ECR |
| **RBAC** | æœ€å°æ¬Šé™åŸå‰‡ | Role, RoleBinding |
| | é¿å…ä½¿ç”¨ cluster-admin | è‡ªè¨‚ ClusterRole |
| **ç¶²è·¯** | ä½¿ç”¨ Network Policy | é™åˆ¶ Pod é€šè¨Š |
| | ä½¿ç”¨æœå‹™ç¶²æ ¼ | Istio, Linkerd |
| **è³‡æº** | è¨­å®šè³‡æºé™åˆ¶ | requests/limits |
| | ä½¿ç”¨ LimitRange | é è¨­é™åˆ¶ |
| **å¯†é‘°** | åŠ å¯† Secrets | KMS, Sealed Secrets |
| | è¼ªæ›å¯†é‘° | å®šæœŸæ›´æ–° |
| **å¯©è¨ˆ** | å•Ÿç”¨å¯©è¨ˆæ—¥èªŒ | Audit Policy |
| | ç›£æ§ç•°å¸¸è¡Œç‚º | Falco |

### é«˜å¯ç”¨æ€§é…ç½®

**å¤šå‰¯æœ¬éƒ¨ç½²ï¼š**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: critical-app
spec:
  replicas: 5
  strategy:
    rollingUpdate:
      maxSurge: 2
      maxUnavailable: 1
  template:
    spec:
      # Pod åè¦ªå’Œæ€§
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - critical-app
            topologyKey: kubernetes.io/hostname
      # å„ªå…ˆç´š
      priorityClassName: high-priority
      # ä¸­æ–·é ç®—
      terminationGracePeriodSeconds: 60
```

**Pod Disruption Budgetï¼š**

```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: critical-app-pdb
spec:
  minAvailable: 3
  selector:
    matchLabels:
      app: critical-app
```

### è³‡æºé…é¡ç®¡ç†

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: production-quota
  namespace: production
spec:
  hard:
    requests.cpu: "100"
    requests.memory: 200Gi
    limits.cpu: "200"
    limits.memory: 400Gi
    persistentvolumeclaims: "20"
    pods: "100"
    services: "50"
    configmaps: "50"
    secrets: "50"

---
apiVersion: v1
kind: LimitRange
metadata:
  name: production-limits
  namespace: production
spec:
  limits:
  # Pod é™åˆ¶
  - max:
      cpu: "4"
      memory: 8Gi
    min:
      cpu: 100m
      memory: 128Mi
    type: Pod
  # Container é™åˆ¶
  - default:
      cpu: 500m
      memory: 512Mi
    defaultRequest:
      cpu: 250m
      memory: 256Mi
    max:
      cpu: "2"
      memory: 4Gi
    min:
      cpu: 50m
      memory: 64Mi
    type: Container
  # PVC é™åˆ¶
  - max:
      storage: 100Gi
    min:
      storage: 1Gi
    type: PersistentVolumeClaim
```

### å‚™ä»½èˆ‡ç½é›£æ¢å¾©

**Velero å‚™ä»½ï¼š**

```bash
# å®‰è£ Velero
velero install \
  --provider aws \
  --plugins velero/velero-plugin-for-aws:v1.7.0 \
  --bucket velero-backups \
  --backup-location-config region=us-west-2 \
  --snapshot-location-config region=us-west-2 \
  --secret-file ./credentials-velero

# å‰µå»ºå‚™ä»½
velero backup create full-backup --include-namespaces production
velero backup create daily-backup --schedule="0 2 * * *"

# é‚„åŸå‚™ä»½
velero restore create --from-backup full-backup

# æŸ¥çœ‹å‚™ä»½
velero backup get
velero restore get
```

## ğŸ“Š ç¸½çµèˆ‡æª¢æŸ¥æ¸…å–®

### æ ¸å¿ƒçŸ¥è­˜å›é¡§

æœ¬ç³»åˆ—ä¸‰ç¯‡æ–‡ç« å®Œæ•´æ¶µè“‹äº† Kubernetes å¾å…¥é–€åˆ°ç”Ÿç”¢ï¼š

**ç¬¬ä¸€ç¯‡ï¼šåŸºç¤æ¦‚å¿µ**
- K8s æ¶æ§‹èˆ‡å…ƒä»¶
- æ ¸å¿ƒè³‡æºæ¦‚å¿µ
- å®‰è£èˆ‡é…ç½®

**ç¬¬äºŒç¯‡ï¼šæ ¸å¿ƒè³‡æºæ“ä½œ**
- kubectl æŒ‡ä»¤å¤§å…¨
- Podã€Deploymentã€Service
- Ingressã€Volumeã€ConfigMap

**ç¬¬ä¸‰ç¯‡ï¼šé€²éšåŠŸèƒ½**ï¼ˆæœ¬ç¯‡ï¼‰
- è‡ªå‹•æ“´å±•ï¼ˆHPA/VPA/CAï¼‰
- RBAC æ¬Šé™ç®¡ç†
- Network Policy
- Helm å¥—ä»¶ç®¡ç†
- ç›£æ§å‘Šè­¦
- æ—¥èªŒæ”¶é›†
- CI/CD æ•´åˆ
- ç”Ÿç”¢æœ€ä½³å¯¦è¸

### ç”Ÿç”¢ç’°å¢ƒæª¢æŸ¥æ¸…å–®

#### ğŸ“‹ éƒ¨ç½²å‰æª¢æŸ¥

- [ ] é…ç½® RBAC æ¬Šé™
- [ ] è¨­å®š Network Policy
- [ ] é…ç½®è³‡æºé™åˆ¶ï¼ˆrequests/limitsï¼‰
- [ ] è¨­å®š PodDisruptionBudget
- [ ] é…ç½®å¥åº·æª¢æŸ¥ï¼ˆliveness/readinessï¼‰
- [ ] è¨­å®š HPA è‡ªå‹•æ“´å±•
- [ ] é…ç½®å¤šå‰¯æœ¬é«˜å¯ç”¨
- [ ] ä½¿ç”¨ Pod åè¦ªå’Œæ€§

#### ğŸ” å®‰å…¨æ€§æª¢æŸ¥

- [ ] æƒææ˜ åƒæ¼æ´
- [ ] åŠ å¯† Secrets
- [ ] é™åˆ¶ç‰¹æ¬Šå®¹å™¨
- [ ] é…ç½®å®‰å…¨ä¸Šä¸‹æ–‡
- [ ] å•Ÿç”¨å¯©è¨ˆæ—¥èªŒ
- [ ] å®šæœŸè¼ªæ›å¯†é‘°
- [ ] ä½¿ç”¨ç§æœ‰ Registry

#### ğŸ“Š ç›£æ§èˆ‡æ—¥èªŒ

- [ ] éƒ¨ç½² Prometheus + Grafana
- [ ] é…ç½®å‘Šè­¦è¦å‰‡
- [ ] éƒ¨ç½²æ—¥èªŒæ”¶é›†ç³»çµ±
- [ ] è¨­å®šæ—¥èªŒä¿ç•™ç­–ç•¥
- [ ] é…ç½®å„€è¡¨æ¿
- [ ] è¨­å®šå‘Šè­¦é€šçŸ¥

#### ğŸ”„ å‚™ä»½èˆ‡æ¢å¾©

- [ ] é…ç½® etcd å‚™ä»½
- [ ] è¨­å®šè³‡æºå‚™ä»½ç­–ç•¥
- [ ] æ¸¬è©¦ç½é›£æ¢å¾©æµç¨‹
- [ ] æ–‡ä»¶åŒ–æ¢å¾©æ­¥é©Ÿ

### å­¸ç¿’è³‡æºæ¨è–¦

**èªè­‰è€ƒè©¦ï¼š**
- CKAï¼ˆç®¡ç†å“¡ï¼‰
- CKADï¼ˆé–‹ç™¼è€…ï¼‰
- CKSï¼ˆå®‰å…¨å°ˆå®¶ï¼‰

**é€²éšå­¸ç¿’ï¼š**
- Service Meshï¼ˆIstioã€Linkerdï¼‰
- Operator Pattern
- GitOpsï¼ˆArgoCDã€Fluxï¼‰
- Serverlessï¼ˆKnativeï¼‰

## ğŸ‰ çµèª

Kubernetes æ˜¯ä¸€å€‹åŠŸèƒ½å¼·å¤§ä½†è¤‡é›œçš„å¹³å°ã€‚é€éæœ¬ç³»åˆ—ä¸‰ç¯‡æ–‡ç« çš„å­¸ç¿’ï¼Œæ‚¨å·²ç¶“æŒæ¡äº†ï¼š

1. **åŸºç¤çŸ¥è­˜**ï¼šç†è§£ K8s æ¶æ§‹èˆ‡æ ¸å¿ƒæ¦‚å¿µ
2. **å¯¦å‹™æ“ä½œ**ï¼šç†Ÿç·´ä½¿ç”¨ kubectl ç®¡ç†è³‡æº
3. **é€²éšæŠ€èƒ½**ï¼šæŒæ¡ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²èˆ‡æœ€ä½³å¯¦è¸

### ä¸‹ä¸€æ­¥å»ºè­°

- **å¯¦è¸å°ˆæ¡ˆ**ï¼šåœ¨å¯¦éš›é …ç›®ä¸­æ‡‰ç”¨ K8s
- **æ·±å…¥å­¸ç¿’**ï¼šæ¢ç´¢ Service Meshã€Operator
- **ç¤¾ç¾¤åƒèˆ‡**ï¼šåƒèˆ‡é–‹æºé …ç›®ï¼Œåˆ†äº«ç¶“é©—
- **æŒçºŒå„ªåŒ–**ï¼šé—œæ³¨æ•ˆèƒ½ã€å®‰å…¨æ€§ã€æˆæœ¬

Kubernetes çš„å­¸ç¿’æ˜¯æŒçºŒçš„éç¨‹ï¼Œéš¨è‘—å¯¦è¸ç¶“é©—çš„ç´¯ç©ï¼Œæ‚¨å°‡èƒ½å¤ æ§‹å»ºæ›´å¼·å¤§ã€æ›´å¯é çš„é›²åŸç”Ÿæ‡‰ç”¨å¹³å°ï¼

ç¥æ‚¨åœ¨é›²åŸç”ŸæŠ€è¡“çš„é“è·¯ä¸Šä¸æ–·é€²æ­¥ï¼ğŸš€
