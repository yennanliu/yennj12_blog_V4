---
title: "Kubernetes Autoscaling Complete Guide (Part 8): Security, Compliance & Governance"
date: 2025-11-10T02:00:00+08:00
draft: false
authors: ["yennj12 team"]
categories: ["all", "engineering", "devops", "kubernetes", "security"]
tags: ["Kubernetes", "K8S", "Security", "RBAC", "Compliance", "Governance", "Policy", "Audit", "Multi-Tenancy", "OPA", "Gatekeeper", "PCI-DSS", "HIPAA", "SOC2"]
summary: "Part 8 of the Kubernetes Autoscaling series: Complete guide to securing autoscaling infrastructure with RBAC, policy enforcement, compliance frameworks (PCI-DSS, HIPAA, SOC2), multi-tenancy patterns, audit logging, and governance best practices for enterprise Kubernetes."
readTime: "50 min"
---

## Series Overview

This is **Part 8** (Final) of the Kubernetes Autoscaling Complete Guide series:

- **[Part 1: Horizontal Pod Autoscaler](./kubernetes-autoscaling-complete-guide-part1-horizontal-pod-autoscaler.md)** - Application-level autoscaling theory
- **[Part 2: Cluster Autoscaling & Cloud Providers](./kubernetes-autoscaling-complete-guide-part2-cluster-autoscaling.md)** - Infrastructure-level autoscaling
- **[Part 3: Hands-On HPA Demo](./kubernetes-autoscaling-complete-guide-part3-hands-on-hpa-demo.md)** - Practical implementation
- **[Part 4: Monitoring, Alerting & Threshold Tuning](./kubernetes-autoscaling-complete-guide-part4-monitoring-alerting.md)** - Production observability
- **[Part 5: VPA & Resource Optimization](./kubernetes-autoscaling-complete-guide-part5-vpa-resource-optimization.md)** - Right-sizing strategies
- **[Part 6: Advanced Autoscaling Patterns](./kubernetes-autoscaling-complete-guide-part6-advanced-patterns.md)** - Complex architectures
- **[Part 7: Production Troubleshooting & War Stories](./kubernetes-autoscaling-complete-guide-part7-troubleshooting-war-stories.md)** - Real-world incidents
- **Part 8 (This Post)**: Security, Compliance & Governance - Enterprise-grade security

---

Autoscaling without proper security and governance is a vulnerability multiplier. A misconfigured HPA can scale to thousands of pods, exhausting budgets. Unauthorized VPA modifications can crash production. This final guide establishes enterprise-grade security, compliance frameworks, and governance for Kubernetes autoscaling.

## The Security Challenge

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           AUTOSCALING SECURITY ATTACK SURFACE                  â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  THREATS                                                  â”‚  â”‚
â”‚  â”‚                                                            â”‚  â”‚
â”‚  â”‚  1. Unauthorized Scaling                                  â”‚  â”‚
â”‚  â”‚     â€¢ Developer scales production to 1000 pods           â”‚  â”‚
â”‚  â”‚     â€¢ Cost: $50,000/month unexpected                     â”‚  â”‚
â”‚  â”‚                                                            â”‚  â”‚
â”‚  â”‚  2. Resource Exhaustion Attack                           â”‚  â”‚
â”‚  â”‚     â€¢ Malicious HPA maxReplicas: 10000                   â”‚  â”‚
â”‚  â”‚     â€¢ Cluster overwhelmed, legitimate workloads starve   â”‚  â”‚
â”‚  â”‚                                                            â”‚  â”‚
â”‚  â”‚  3. Privilege Escalation via VPA                         â”‚  â”‚
â”‚  â”‚     â€¢ VPA grants excessive CPU/memory                    â”‚  â”‚
â”‚  â”‚     â€¢ Pod escapes resource limits                         â”‚  â”‚
â”‚  â”‚                                                            â”‚  â”‚
â”‚  â”‚  4. Compliance Violations                                â”‚  â”‚
â”‚  â”‚     â€¢ PCI workload auto-scales to non-compliant zone     â”‚  â”‚
â”‚  â”‚     â€¢ Audit logs missing autoscaling decisions           â”‚  â”‚
â”‚  â”‚                                                            â”‚  â”‚
â”‚  â”‚  5. Data Exposure                                         â”‚  â”‚
â”‚  â”‚     â€¢ Autoscaled pod in wrong namespace                  â”‚  â”‚
â”‚  â”‚     â€¢ Sensitive data accessed by unauthorized pod        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Part 1: RBAC for Autoscaling

### Principle of Least Privilege

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 RBAC HIERARCHY                             â”‚
â”‚                                                             â”‚
â”‚  Cluster Admin                                             â”‚
â”‚       â†“                                                     â”‚
â”‚  Platform Team (manage autoscaling infrastructure)         â”‚
â”‚       â†“                                                     â”‚
â”‚  SRE Team (view & modify autoscaling in all namespaces)    â”‚
â”‚       â†“                                                     â”‚
â”‚  Team Lead (modify autoscaling in team namespace)          â”‚
â”‚       â†“                                                     â”‚
â”‚  Developer (view autoscaling in team namespace)            â”‚
â”‚       â†“                                                     â”‚
â”‚  Application (ServiceAccount with minimal permissions)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Role Definitions

#### 1. Cluster Admin (Full Control)

```yaml
# Cluster-wide autoscaling admin
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: autoscaling-admin
  labels:
    rbac.authorization.k8s.io/aggregate-to-admin: "true"
rules:
# HPA permissions
- apiGroups: ["autoscaling"]
  resources: ["horizontalpodautoscalers"]
  verbs: ["*"]

# VPA permissions
- apiGroups: ["autoscaling.k8s.io"]
  resources: ["verticalpodautoscalers"]
  verbs: ["*"]

# KEDA permissions
- apiGroups: ["keda.sh"]
  resources: ["scaledobjects", "scaledjobs", "triggerauthentications", "clustertriggerauthentications"]
  verbs: ["*"]

# Cluster Autoscaler config
- apiGroups: [""]
  resources: ["configmaps"]
  resourceNames: ["cluster-autoscaler-status"]
  verbs: ["get", "list", "watch", "update"]

# Karpenter permissions
- apiGroups: ["karpenter.sh"]
  resources: ["nodepools", "nodeclaims"]
  verbs: ["*"]

- apiGroups: ["karpenter.k8s.aws"]
  resources: ["ec2nodeclasses"]
  verbs: ["*"]

# Metrics (for debugging)
- apiGroups: ["metrics.k8s.io"]
  resources: ["pods", "nodes"]
  verbs: ["get", "list"]

---
# Bind to platform team
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: platform-team-autoscaling-admin
subjects:
- kind: Group
  name: platform-team
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: autoscaling-admin
  apiGroup: rbac.authorization.k8s.io
```

#### 2. SRE Team (View + Modify in All Namespaces)

```yaml
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: autoscaling-operator
rules:
# HPA management
- apiGroups: ["autoscaling"]
  resources: ["horizontalpodautoscalers"]
  verbs: ["get", "list", "watch", "update", "patch"]

# VPA view-only (modifications require approval)
- apiGroups: ["autoscaling.k8s.io"]
  resources: ["verticalpodautoscalers"]
  verbs: ["get", "list", "watch"]

# KEDA view
- apiGroups: ["keda.sh"]
  resources: ["scaledobjects", "scaledjobs"]
  verbs: ["get", "list", "watch"]

# View deployments/statefulsets (for context)
- apiGroups: ["apps"]
  resources: ["deployments", "statefulsets", "replicasets"]
  verbs: ["get", "list", "watch"]

# View pods and nodes
- apiGroups: [""]
  resources: ["pods", "nodes"]
  verbs: ["get", "list", "watch"]

# Metrics access
- apiGroups: ["metrics.k8s.io"]
  resources: ["pods", "nodes"]
  verbs: ["get", "list"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: sre-team-autoscaling-operator
subjects:
- kind: Group
  name: sre-team
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: autoscaling-operator
  apiGroup: rbac.authorization.k8s.io
```

#### 3. Team Lead (Namespace-Scoped)

```yaml
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: autoscaling-manager
  namespace: team-a
rules:
# HPA management within namespace
- apiGroups: ["autoscaling"]
  resources: ["horizontalpodautoscalers"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]

# VPA management with restrictions (via admission webhook)
- apiGroups: ["autoscaling.k8s.io"]
  resources: ["verticalpodautoscalers"]
  verbs: ["get", "list", "watch", "create", "update", "patch"]

# KEDA management
- apiGroups: ["keda.sh"]
  resources: ["scaledobjects", "triggerauthentications"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]

# View workloads
- apiGroups: ["apps"]
  resources: ["deployments", "statefulsets"]
  verbs: ["get", "list", "watch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: team-lead-autoscaling-manager
  namespace: team-a
subjects:
- kind: User
  name: alice@example.com
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: autoscaling-manager
  apiGroup: rbac.authorization.k8s.io
```

#### 4. Developer (View-Only)

```yaml
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: autoscaling-viewer
  namespace: team-a
rules:
# View HPA
- apiGroups: ["autoscaling"]
  resources: ["horizontalpodautoscalers"]
  verbs: ["get", "list", "watch"]

# View VPA
- apiGroups: ["autoscaling.k8s.io"]
  resources: ["verticalpodautoscalers"]
  verbs: ["get", "list", "watch"]

# View KEDA
- apiGroups: ["keda.sh"]
  resources: ["scaledobjects", "scaledjobs"]
  verbs: ["get", "list", "watch"]

# View metrics
- apiGroups: ["metrics.k8s.io"]
  resources: ["pods"]
  verbs: ["get", "list"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: developers-autoscaling-viewer
  namespace: team-a
subjects:
- kind: Group
  name: developers
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: autoscaling-viewer
  apiGroup: rbac.authorization.k8s.io
```

#### 5. Service Account (Application)

```yaml
---
# Minimal permissions for application pod
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-app
  namespace: production
automountServiceAccountToken: true

---
# No autoscaling permissions by default
# Applications should NOT modify their own autoscaling
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: my-app-role
  namespace: production
rules:
# Only allow reading own pod info
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list"]
  # Restrict to own pod
  resourceNames: [] # Enforced by admission controller

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: my-app-binding
  namespace: production
subjects:
- kind: ServiceAccount
  name: my-app
  namespace: production
roleRef:
  kind: Role
  name: my-app-role
  apiGroup: rbac.authorization.k8s.io
```

### Special Use Case: CI/CD Pipeline

```yaml
---
# ServiceAccount for CI/CD to update HPA during deployments
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cicd-deployer
  namespace: production

---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: cicd-autoscaling-deployer
  namespace: production
rules:
# Allow creating/updating HPA during deployment
- apiGroups: ["autoscaling"]
  resources: ["horizontalpodautoscalers"]
  verbs: ["get", "list", "create", "update", "patch"]

# Allow updating VPA (but not deleting)
- apiGroups: ["autoscaling.k8s.io"]
  resources: ["verticalpodautoscalers"]
  verbs: ["get", "list", "create", "update", "patch"]

# Manage deployments
- apiGroups: ["apps"]
  resources: ["deployments"]
  verbs: ["get", "list", "update", "patch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: cicd-autoscaling-deployer-binding
  namespace: production
subjects:
- kind: ServiceAccount
  name: cicd-deployer
  namespace: production
roleRef:
  kind: Role
  name: cicd-autoscaling-deployer
  apiGroup: rbac.authorization.k8s.io
```

## Part 2: Policy Enforcement with OPA/Gatekeeper

### Install Gatekeeper

```bash
# Install Gatekeeper
kubectl apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper/release-3.14/deploy/gatekeeper.yaml

# Verify installation
kubectl get pods -n gatekeeper-system

# Check CRDs
kubectl get crd | grep gatekeeper
```

### Policy 1: Enforce HPA Replica Limits

**Constraint Template:**

```yaml
apiVersion: templates.gatekeeper.sh/v1
kind: ConstraintTemplate
metadata:
  name: k8shpareplicalimits
  annotations:
    description: "Enforces minimum and maximum replica limits for HPAs"
spec:
  crd:
    spec:
      names:
        kind: K8sHpaReplicaLimits
      validation:
        openAPIV3Schema:
          type: object
          properties:
            minReplicas:
              type: integer
              description: "Minimum allowed minReplicas"
            maxReplicas:
              type: integer
              description: "Maximum allowed maxReplicas"
            maxReplicasPerNamespace:
              type: integer
              description: "Maximum total replicas across all HPAs in namespace"

  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8shpareplicalimits

        violation[{"msg": msg}] {
          input.review.kind.kind == "HorizontalPodAutoscaler"
          hpa := input.review.object.spec

          # Check minReplicas
          hpa.minReplicas < input.parameters.minReplicas
          msg := sprintf("HPA minReplicas (%v) is below the allowed minimum (%v)", [hpa.minReplicas, input.parameters.minReplicas])
        }

        violation[{"msg": msg}] {
          input.review.kind.kind == "HorizontalPodAutoscaler"
          hpa := input.review.object.spec

          # Check maxReplicas
          hpa.maxReplicas > input.parameters.maxReplicas
          msg := sprintf("HPA maxReplicas (%v) exceeds the allowed maximum (%v)", [hpa.maxReplicas, input.parameters.maxReplicas])
        }

        violation[{"msg": msg}] {
          input.review.kind.kind == "HorizontalPodAutoscaler"
          hpa := input.review.object.spec

          # Check ratio
          ratio := hpa.maxReplicas / hpa.minReplicas
          ratio > 10
          msg := sprintf("HPA maxReplicas/minReplicas ratio (%v) exceeds 10x", [ratio])
        }
```

**Constraint Instance:**

```yaml
---
# Apply limits to production namespace
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sHpaReplicaLimits
metadata:
  name: production-hpa-limits
spec:
  match:
    kinds:
      - apiGroups: ["autoscaling"]
        kinds: ["HorizontalPodAutoscaler"]
    namespaces:
      - production
  parameters:
    minReplicas: 2      # At least 2 replicas
    maxReplicas: 100    # Max 100 replicas
    maxReplicasPerNamespace: 500  # Total across all HPAs

---
# More permissive for staging
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sHpaReplicaLimits
metadata:
  name: staging-hpa-limits
spec:
  match:
    kinds:
      - apiGroups: ["autoscaling"]
        kinds: ["HorizontalPodAutoscaler"]
    namespaces:
      - staging
  parameters:
    minReplicas: 1
    maxReplicas: 50
```

### Policy 2: Enforce VPA Resource Boundaries

```yaml
apiVersion: templates.gatekeeper.sh/v1
kind: ConstraintTemplate
metadata:
  name: k8svparesourcelimits
spec:
  crd:
    spec:
      names:
        kind: K8sVpaResourceLimits
      validation:
        openAPIV3Schema:
          type: object
          properties:
            maxCpu:
              type: string
            maxMemory:
              type: string

  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8svparesourcelimits

        import future.keywords.in

        violation[{"msg": msg}] {
          input.review.kind.kind == "VerticalPodAutoscaler"
          vpa := input.review.object.spec

          # Check each container policy
          container := vpa.resourcePolicy.containerPolicies[_]

          # Parse CPU limits
          maxCpu := parse_cpu(container.maxAllowed.cpu)
          allowedMaxCpu := parse_cpu(input.parameters.maxCpu)

          maxCpu > allowedMaxCpu
          msg := sprintf("VPA maxAllowed CPU (%v) exceeds limit (%v)", [container.maxAllowed.cpu, input.parameters.maxCpu])
        }

        violation[{"msg": msg}] {
          input.review.kind.kind == "VerticalPodAutoscaler"
          vpa := input.review.object.spec

          container := vpa.resourcePolicy.containerPolicies[_]

          # Parse memory limits
          maxMemory := parse_memory(container.maxAllowed.memory)
          allowedMaxMemory := parse_memory(input.parameters.maxMemory)

          maxMemory > allowedMaxMemory
          msg := sprintf("VPA maxAllowed memory (%v) exceeds limit (%v)", [container.maxAllowed.memory, input.parameters.maxMemory])
        }

        # Helper functions
        parse_cpu(cpu) = value {
          # Convert CPU to millicores
          endswith(cpu, "m")
          value := to_number(trim_suffix(cpu, "m"))
        }

        parse_cpu(cpu) = value {
          # Cores to millicores
          not endswith(cpu, "m")
          value := to_number(cpu) * 1000
        }

        parse_memory(mem) = value {
          # Convert memory to bytes
          endswith(mem, "Gi")
          value := to_number(trim_suffix(mem, "Gi")) * 1024 * 1024 * 1024
        }

        parse_memory(mem) = value {
          endswith(mem, "Mi")
          value := to_number(trim_suffix(mem, "Mi")) * 1024 * 1024
        }

---
# Apply VPA limits
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sVpaResourceLimits
metadata:
  name: vpa-resource-limits
spec:
  match:
    kinds:
      - apiGroups: ["autoscaling.k8s.io"]
        kinds: ["VerticalPodAutoscaler"]
  parameters:
    maxCpu: "16"      # Max 16 cores
    maxMemory: "64Gi" # Max 64GB
```

### Policy 3: Require Resource Requests for HPA

```yaml
apiVersion: templates.gatekeeper.sh/v1
kind: ConstraintTemplate
metadata:
  name: k8shparequiresresourcerequests
spec:
  crd:
    spec:
      names:
        kind: K8sHpaRequiresResourceRequests

  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8shparequiresresourcerequests

        violation[{"msg": msg}] {
          input.review.kind.kind == "HorizontalPodAutoscaler"
          hpa := input.review.object

          # Get target deployment
          targetRef := hpa.spec.scaleTargetRef

          # Check if CPU metric is used
          some i
          hpa.spec.metrics[i].type == "Resource"
          hpa.spec.metrics[i].resource.name == "cpu"

          # Get the target deployment (this is simplified)
          msg := "HPA using CPU metric requires target workload to have CPU resource requests defined"
        }

---
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sHpaRequiresResourceRequests
metadata:
  name: hpa-requires-requests
spec:
  match:
    kinds:
      - apiGroups: ["autoscaling"]
        kinds: ["HorizontalPodAutoscaler"]
```

### Policy 4: Enforce Namespace Quotas

```yaml
# ResourceQuota for autoscaling
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: autoscaling-quota
  namespace: team-a
spec:
  hard:
    # Limit number of autoscaling objects
    count/horizontalpodautoscalers.autoscaling: "20"
    count/verticalpodautoscalers.autoscaling.k8s.io: "10"

    # Total resource limits across all pods
    limits.cpu: "100"
    limits.memory: "200Gi"
    requests.cpu: "50"
    requests.memory: "100Gi"

    # Pod count limits (affects HPA max)
    pods: "500"

---
# LimitRange to set defaults
apiVersion: v1
kind: LimitRange
metadata:
  name: autoscaling-limits
  namespace: team-a
spec:
  limits:
  - max:
      cpu: "8"
      memory: "16Gi"
    min:
      cpu: "100m"
      memory: "128Mi"
    type: Container

  - max:
      cpu: "16"
      memory: "32Gi"
    type: Pod
```

## Part 3: Multi-Tenancy Patterns

### Pattern 1: Namespace Isolation

```yaml
# Namespace with strict boundaries
---
apiVersion: v1
kind: Namespace
metadata:
  name: tenant-a
  labels:
    tenant: tenant-a
    environment: production
    compliance: pci-dss

---
# Network Policy: isolate namespace
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-all-ingress
  namespace: tenant-a
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress

---
# ResourceQuota per tenant
apiVersion: v1
kind: ResourceQuota
metadata:
  name: tenant-a-quota
  namespace: tenant-a
spec:
  hard:
    requests.cpu: "50"
    requests.memory: "100Gi"
    limits.cpu: "100"
    limits.memory: "200Gi"
    pods: "500"
    count/horizontalpodautoscalers.autoscaling: "20"

---
# PodSecurityStandard
apiVersion: v1
kind: Namespace
metadata:
  name: tenant-a
  labels:
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted
```

### Pattern 2: Node Isolation

```yaml
# Dedicated node pool for tenant
---
apiVersion: v1
kind: Node
metadata:
  name: node-tenant-a-1
  labels:
    tenant: tenant-a
    node-pool: tenant-a-dedicated
  spec:
    taints:
    - key: tenant
      value: tenant-a
      effect: NoSchedule

---
# Karpenter NodePool for tenant
apiVersion: karpenter.sh/v1beta1
kind: NodePool
metadata:
  name: tenant-a-pool
spec:
  template:
    metadata:
      labels:
        tenant: tenant-a
    spec:
      requirements:
      - key: tenant
        operator: In
        values: ["tenant-a"]

      taints:
      - key: tenant
        value: tenant-a
        effect: NoSchedule

      nodeClassRef:
        name: tenant-a-nodes

  limits:
    cpu: "100"
    memory: "200Gi"

---
# Tenant workload with affinity
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tenant-a-app
  namespace: tenant-a
spec:
  template:
    spec:
      # Force scheduling on tenant nodes
      nodeSelector:
        tenant: tenant-a

      tolerations:
      - key: tenant
        value: tenant-a
        effect: NoSchedule

      # Anti-affinity to spread across nodes
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values: ["tenant-a-app"]
            topologyKey: kubernetes.io/hostname
```

### Pattern 3: Hierarchical Namespaces (HNC)

```yaml
# Install Hierarchical Namespace Controller
---
apiVersion: v1
kind: Namespace
metadata:
  name: organization-a

---
# Parent namespace with autoscaling policies
apiVersion: hnc.x-k8s.io/v1alpha2
kind: HierarchyConfiguration
metadata:
  name: hierarchy
  namespace: organization-a
spec:
  parent: ""  # Root namespace

---
# Child namespace inherits policies
apiVersion: v1
kind: Namespace
metadata:
  name: org-a-team-1
  labels:
    organization: organization-a

---
apiVersion: hnc.x-k8s.io/v1alpha2
kind: HierarchyConfiguration
metadata:
  name: hierarchy
  namespace: org-a-team-1
spec:
  parent: organization-a

---
# RoleBinding in parent propagates to children
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: org-a-autoscaling-policy
  namespace: organization-a
  labels:
    hnc.x-k8s.io/propagate: "true"  # Propagate to children
subjects:
- kind: Group
  name: org-a-admins
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: autoscaling-manager
  apiGroup: rbac.authorization.k8s.io
```

## Part 4: Compliance Frameworks

### PCI-DSS Compliance

**Requirements for Autoscaling:**

```yaml
# 1. Requirement 2.2: Secure configurations
---
apiVersion: templates.gatekeeper.sh/v1
kind: ConstraintTemplate
metadata:
  name: pcidssautoscalingsecurity
spec:
  crd:
    spec:
      names:
        kind: PciDssAutoscalingSecurity

  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package pcidssautoscalingsecurity

        violation[{"msg": msg}] {
          input.review.kind.kind == "HorizontalPodAutoscaler"
          hpa := input.review.object

          # Check if in PCI namespace
          input.review.namespace == "pci-workloads"

          # Ensure minimum 2 replicas (high availability)
          hpa.spec.minReplicas < 2
          msg := "PCI-DSS: minReplicas must be at least 2 for high availability"
        }

        violation[{"msg": msg}] {
          input.review.kind.kind == "Deployment"
          deployment := input.review.object

          # PCI workloads must be in specific zones
          input.review.namespace == "pci-workloads"

          not deployment.spec.template.spec.nodeSelector["compliance-zone"]
          msg := "PCI-DSS: Workloads must run in compliance-certified zones"
        }

---
# 2. Requirement 10: Audit logging
apiVersion: audit.k8s.io/v1
kind: Policy
metadata:
  name: autoscaling-audit-policy
rules:
# Log all autoscaling changes
- level: RequestResponse
  omitStages:
  - RequestReceived
  resources:
  - group: "autoscaling"
    resources: ["horizontalpodautoscalers"]
  - group: "autoscaling.k8s.io"
    resources: ["verticalpodautoscalers"]
  - group: "keda.sh"
    resources: ["scaledobjects", "scaledjobs"]

# Log all scaling events
- level: Metadata
  resources:
  - group: ""
    resources: ["events"]
  namespaces: ["pci-workloads"]

---
# 3. Requirement 7: Access control
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: pci-autoscaling-restricted
  namespace: pci-workloads
rules:
# Read-only for most users
- apiGroups: ["autoscaling"]
  resources: ["horizontalpodautoscalers"]
  verbs: ["get", "list", "watch"]

# No VPA modifications in PCI namespace
- apiGroups: ["autoscaling.k8s.io"]
  resources: ["verticalpodautoscalers"]
  verbs: ["get", "list", "watch"]

---
# 4. PCI-compliant namespace
apiVersion: v1
kind: Namespace
metadata:
  name: pci-workloads
  labels:
    compliance: pci-dss
    security-level: high
    audit: enabled
```

### HIPAA Compliance

```yaml
# HIPAA-compliant autoscaling
---
# 1. Encryption in transit and at rest
apiVersion: v1
kind: Namespace
metadata:
  name: hipaa-workloads
  labels:
    compliance: hipaa
    encryption: required
  annotations:
    # Enforce mTLS via service mesh
    linkerd.io/inject: enabled

---
# 2. Node pool with encryption
apiVersion: karpenter.sh/v1beta1
kind: NodePool
metadata:
  name: hipaa-nodes
spec:
  template:
    spec:
      requirements:
      - key: compliance
        operator: In
        values: ["hipaa"]

      nodeClassRef:
        name: hipaa-encrypted

---
apiVersion: karpenter.k8s.aws/v1beta1
kind: EC2NodeClass
metadata:
  name: hipaa-encrypted
spec:
  amiFamily: AL2
  role: KarpenterNodeRole-hipaa

  blockDeviceMappings:
  - deviceName: /dev/xvda
    ebs:
      volumeSize: 100Gi
      volumeType: gp3
      encrypted: true      # HIPAA: Encryption at rest
      kmsKeyID: "arn:aws:kms:us-east-1:123456789:key/abcd-1234"
      deleteOnTermination: true

  metadataOptions:
    httpTokens: required  # HIPAA: Secure metadata access

  userData: |
    #!/bin/bash
    # HIPAA: Audit logging
    auditctl -w /etc/kubernetes -p wa -k kubernetes_config

---
# 3. HPA policy for HIPAA workloads
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: K8sHpaReplicaLimits
metadata:
  name: hipaa-hpa-limits
spec:
  match:
    kinds:
      - apiGroups: ["autoscaling"]
        kinds: ["HorizontalPodAutoscaler"]
    namespaces:
      - hipaa-workloads
  parameters:
    minReplicas: 3      # HIPAA: High availability
    maxReplicas: 50

---
# 4. Data residency constraint
apiVersion: templates.gatekeeper.sh/v1
kind: ConstraintTemplate
metadata:
  name: hipaadataresidency
spec:
  crd:
    spec:
      names:
        kind: HipaaDataResidency

  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package hipaadataresidency

        violation[{"msg": msg}] {
          input.review.kind.kind in ["Deployment", "StatefulSet"]
          input.review.namespace == "hipaa-workloads"

          # Must specify allowed regions
          not input.review.object.spec.template.spec.nodeSelector["topology.kubernetes.io/region"]

          msg := "HIPAA: Workloads must specify allowed regions for data residency"
        }

---
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: HipaaDataResidency
metadata:
  name: hipaa-data-residency
spec:
  match:
    kinds:
      - apiGroups: ["apps"]
        kinds: ["Deployment", "StatefulSet"]
    namespaces:
      - hipaa-workloads
```

### SOC 2 Compliance

```yaml
# SOC 2 compliance for autoscaling
---
# 1. Change management process
apiVersion: v1
kind: ConfigMap
metadata:
  name: autoscaling-change-policy
  namespace: compliance
data:
  policy.yaml: |
    # All autoscaling changes require:
    # 1. Peer review (enforced by Git)
    # 2. Approval from team lead
    # 3. Testing in staging
    # 4. Audit log entry

---
# 2. Audit logging to external system
apiVersion: v1
kind: ConfigMap
metadata:
  name: audit-webhook-config
  namespace: kube-system
data:
  webhook.yaml: |
    apiVersion: v1
    kind: Config
    clusters:
    - name: audit-sink
      cluster:
        server: https://audit-collector.example.com/k8s-audit
        certificate-authority: /etc/audit/ca.crt
    users:
    - name: audit-sink
      user:
        client-certificate: /etc/audit/client.crt
        client-key: /etc/audit/client.key
    contexts:
    - context:
        cluster: audit-sink
        user: audit-sink
      name: default-context
    current-context: default-context

---
# 3. Monitoring and alerting
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: soc2-autoscaling-compliance
  namespace: monitoring
spec:
  groups:
  - name: soc2-compliance
    interval: 1m
    rules:
    # Alert on unauthorized changes
    - alert: UnauthorizedAutoscalingChange
      expr: |
        increase(apiserver_audit_event_total{
          verb="create|update|delete",
          objectRef_resource=~"horizontalpodautoscalers|verticalpodautoscalers",
          user_username!~"system:.*|approved-users"
        }[5m]) > 0
      labels:
        severity: critical
        compliance: soc2
      annotations:
        summary: "Unauthorized autoscaling modification detected"
        description: "User {{ $labels.user_username }} modified autoscaling without authorization"

    # Alert on excessive scaling
    - alert: ExcessiveScaling
      expr: |
        rate(kube_horizontalpodautoscaler_status_current_replicas[5m]) > 2
      for: 10m
      labels:
        severity: warning
        compliance: soc2
      annotations:
        summary: "Excessive autoscaling activity detected"

---
# 4. Separation of duties
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: autoscaling-approver
rules:
# Approvers can only approve, not create
- apiGroups: ["autoscaling"]
  resources: ["horizontalpodautoscalers"]
  verbs: ["get", "list", "watch"]

- apiGroups: ["autoscaling"]
  resources: ["horizontalpodautoscalers/status"]
  verbs: ["update", "patch"]
```

## Part 5: Audit Logging and Forensics

### Comprehensive Audit Policy

```yaml
apiVersion: audit.k8s.io/v1
kind: Policy
metadata:
  name: comprehensive-audit-policy
rules:
# 1. Log all autoscaling object changes
- level: RequestResponse
  omitStages:
  - RequestReceived
  resources:
  - group: "autoscaling"
    resources: ["horizontalpodautoscalers"]
  - group: "autoscaling.k8s.io"
    resources: ["verticalpodautoscalers"]
  - group: "keda.sh"
    resources: ["scaledobjects", "scaledjobs", "triggerauthentications"]
  - group: "karpenter.sh"
    resources: ["nodepools", "nodeclaims"]

# 2. Log scaling events
- level: Metadata
  resources:
  - group: ""
    resources: ["events"]
  namespaceSelector:
    matchNames:
    - production
    - staging

# 3. Log deployment changes (related to autoscaling)
- level: Request
  verbs: ["update", "patch"]
  resources:
  - group: "apps"
    resources: ["deployments", "statefulsets"]
  namespaces: ["production"]

# 4. Log node changes (cluster autoscaling)
- level: Metadata
  resources:
  - group: ""
    resources: ["nodes"]

# 5. Log ConfigMap changes (autoscaler configs)
- level: RequestResponse
  resources:
  - group: ""
    resources: ["configmaps"]
  namespaces: ["kube-system"]
  resourceNames:
  - cluster-autoscaler-status
  - karpenter-global-settings

# 6. Log RBAC changes for autoscaling
- level: RequestResponse
  verbs: ["create", "update", "patch", "delete"]
  resources:
  - group: "rbac.authorization.k8s.io"
    resources: ["roles", "rolebindings", "clusterroles", "clusterrolebindings"]
  omitManagedFields: true

# 7. Exclude noisy read-only operations
- level: None
  verbs: ["get", "list", "watch"]
  resources:
  - group: ""
    resources: ["pods", "services"]
```

### Audit Log Aggregation

```yaml
# Fluentd for audit log shipping
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-audit-config
  namespace: kube-system
data:
  fluent.conf: |
    # Read audit logs
    <source>
      @type tail
      path /var/log/kubernetes/kube-apiserver-audit.log
      pos_file /var/log/fluentd-audit.pos
      tag k8s-audit
      <parse>
        @type json
        time_key timestamp
        time_format %Y-%m-%dT%H:%M:%S.%N%z
      </parse>
    </source>

    # Filter for autoscaling events
    <filter k8s-audit>
      @type grep
      <regexp>
        key $.objectRef.resource
        pattern /horizontalpodautoscalers|verticalpodautoscalers|scaledobjects|nodepools/
      </regexp>
    </filter>

    # Enrich with metadata
    <filter k8s-audit>
      @type record_transformer
      enable_ruby true
      <record>
        cluster_name "#{ENV['CLUSTER_NAME']}"
        environment "#{ENV['ENVIRONMENT']}"
        timestamp_unix ${Time.parse(record['timestamp']).to_i}
      </record>
    </filter>

    # Ship to multiple destinations
    <match k8s-audit>
      @type copy

      # Elasticsearch for search
      <store>
        @type elasticsearch
        host elasticsearch.logging.svc
        port 9200
        index_name k8s-audit-%Y%m%d
        type_name audit
      </store>

      # S3 for long-term retention
      <store>
        @type s3
        s3_bucket compliance-audit-logs
        s3_region us-east-1
        path k8s-audit/%Y/%m/%d/
        time_slice_format %Y%m%d%H
        <buffer>
          @type file
          path /var/log/fluentd-s3-buffer/
          timekey 3600
          timekey_wait 10m
        </buffer>
      </store>

      # Splunk for SOC
      <store>
        @type splunk_hec
        host splunk.example.com
        port 8088
        token ${SPLUNK_HEC_TOKEN}
        source k8s-audit
        sourcetype _json
      </store>
    </match>

---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd-audit
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: fluentd-audit
  template:
    metadata:
      labels:
        app: fluentd-audit
    spec:
      serviceAccountName: fluentd
      containers:
      - name: fluentd
        image: fluent/fluentd-kubernetes-daemonset:v1-debian-elasticsearch
        env:
        - name: CLUSTER_NAME
          value: production-cluster
        - name: ENVIRONMENT
          value: production
        - name: SPLUNK_HEC_TOKEN
          valueFrom:
            secretKeyRef:
              name: splunk-credentials
              key: hec-token
        volumeMounts:
        - name: audit-logs
          mountPath: /var/log/kubernetes
          readOnly: true
        - name: config
          mountPath: /fluentd/etc
      volumes:
      - name: audit-logs
        hostPath:
          path: /var/log/kubernetes
      - name: config
        configMap:
          name: fluentd-audit-config
```

### Forensics Queries

```bash
#!/bin/bash
# forensics-queries.sh - Common audit queries

# 1. Who scaled what and when?
kubectl logs -n kube-system -l component=kube-apiserver | \
  jq -r 'select(.objectRef.resource == "horizontalpodautoscalers" and .verb == "update") |
         "\(.requestReceivedTimestamp) | User: \(.user.username) | Namespace: \(.objectRef.namespace) | HPA: \(.objectRef.name)"'

# 2. Find unauthorized scaling attempts
kubectl logs -n kube-system -l component=kube-apiserver | \
  jq -r 'select(.objectRef.resource == "horizontalpodautoscalers" and .responseStatus.code >= 400) |
         "\(.requestReceivedTimestamp) | User: \(.user.username) | Action: \(.verb) | Status: \(.responseStatus.code) | Reason: \(.responseStatus.reason)"'

# 3. Track VPA modifications
kubectl logs -n kube-system -l component=kube-apiserver | \
  jq -r 'select(.objectRef.resource == "verticalpodautoscalers" and .verb in ["create", "update", "patch", "delete"]) |
         "\(.requestReceivedTimestamp) | User: \(.user.username) | Verb: \(.verb) | VPA: \(.objectRef.namespace)/\(.objectRef.name)"'

# 4. Find expensive scaling events
kubectl logs -n kube-system -l component=kube-apiserver | \
  jq -r 'select(.objectRef.resource == "horizontalpodautoscalers") |
         select(.requestObject.spec.maxReplicas > 100) |
         "\(.requestReceivedTimestamp) | User: \(.user.username) | HPA: \(.objectRef.name) | MaxReplicas: \(.requestObject.spec.maxReplicas)"'

# 5. Compliance report: all autoscaling changes in last 24h
kubectl logs -n kube-system -l component=kube-apiserver --since=24h | \
  jq -r 'select(.objectRef.resource in ["horizontalpodautoscalers", "verticalpodautoscalers"]) |
         select(.verb in ["create", "update", "patch", "delete"]) |
         [.requestReceivedTimestamp, .user.username, .verb, .objectRef.resource, .objectRef.namespace, .objectRef.name, .responseStatus.code] |
         @csv' > compliance-report.csv
```

## Part 6: Security Best Practices Checklist

### Pre-Production Checklist

```yaml
# security-checklist.yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: autoscaling-security-checklist
  namespace: compliance
data:
  checklist.md: |
    # Autoscaling Security Checklist

    ## RBAC
    - [ ] Principle of least privilege applied
    - [ ] ServiceAccounts for applications have minimal permissions
    - [ ] CI/CD pipelines use dedicated ServiceAccounts
    - [ ] Regular RBAC audits scheduled

    ## Policy Enforcement
    - [ ] Gatekeeper installed and configured
    - [ ] HPA replica limits enforced
    - [ ] VPA resource boundaries enforced
    - [ ] Namespace quotas configured
    - [ ] PodDisruptionBudgets required

    ## Multi-Tenancy
    - [ ] Namespace isolation configured
    - [ ] Network policies implemented
    - [ ] Node isolation (if required)
    - [ ] Resource quotas per tenant

    ## Compliance
    - [ ] Audit logging enabled
    - [ ] Audit logs shipped to external system
    - [ ] Retention policy configured (7 years for SOX)
    - [ ] Compliance-specific policies enforced (PCI/HIPAA/SOC2)
    - [ ] Regular compliance audits scheduled

    ## Monitoring
    - [ ] Autoscaling metrics collected
    - [ ] Alerts configured for unauthorized changes
    - [ ] Dashboard for compliance team
    - [ ] Incident response runbook documented

    ## Cost Control
    - [ ] Budget alerts configured
    - [ ] Cost attribution labels required
    - [ ] FinOps dashboard deployed
    - [ ] Regular cost reviews scheduled

    ## Disaster Recovery
    - [ ] Autoscaling configs backed up
    - [ ] Restore procedure tested
    - [ ] Failover tested
    - [ ] RTO/RPO documented
```

### Security Scanning

```yaml
# Kubernetes manifest scanning with Kubesec
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: autoscaling-security-scan
  namespace: compliance
spec:
  schedule: "0 2 * * *"  # Daily at 2 AM
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: security-scanner
          containers:
          - name: scanner
            image: kubesec/kubesec:latest
            command:
            - /bin/sh
            - -c
            - |
              #!/bin/sh

              echo "Scanning autoscaling configurations..."

              # Get all HPA configs
              kubectl get hpa -A -o yaml | kubesec scan -

              # Get all VPA configs
              kubectl get vpa -A -o yaml | kubesec scan -

              # Get all Karpenter NodePools
              kubectl get nodepools -o yaml | kubesec scan -

              # Check for common misconfigurations
              echo ""
              echo "=== Common Misconfigurations ==="

              # HPA without resource requests
              kubectl get hpa -A -o json | jq -r '
                .items[] |
                select(.spec.metrics[]? | select(.resource.name == "cpu")) |
                "\(.metadata.namespace)/\(.metadata.name): Check if target has CPU requests"
              '

              # VPA in Auto mode without PDB
              kubectl get vpa -A -o json | jq -r '
                .items[] |
                select(.spec.updatePolicy.updateMode == "Auto") |
                "\(.metadata.namespace)/\(.metadata.name): VPA Auto mode - ensure PDB exists"
              '

              echo ""
              echo "Scan complete. Check logs for issues."

          restartPolicy: OnFailure
```

## Part 7: Incident Response

### Autoscaling Security Incident Runbook

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: incident-response-runbook
  namespace: compliance
data:
  autoscaling-incident.md: |
    # Autoscaling Security Incident Response

    ## Scenario 1: Unauthorized Scaling Detected

    ### Detection
    - Alert: "UnauthorizedAutoscalingChange"
    - Source: Prometheus alert or audit log

    ### Immediate Actions
    1. **Identify the change**
       ```bash
       kubectl get events -A | grep HorizontalPodAutoscaler
       kubectl logs -n kube-system -l component=kube-apiserver | grep horizontalpodautoscalers
       ```

    2. **Identify the user**
       ```bash
       # From audit logs
       kubectl logs -n kube-system -l component=kube-apiserver | \
         jq -r 'select(.objectRef.resource == "horizontalpodautoscalers" and .verb == "update") |
                "\(.user.username) at \(.requestReceivedTimestamp)"'
       ```

    3. **Assess impact**
       - Check current replica count
       - Check cost impact
       - Check service availability

    4. **Contain the incident**
       ```bash
       # Revert HPA to safe state
       kubectl patch hpa <name> -n <namespace> -p '{"spec":{"maxReplicas":10}}'

       # Temporarily disable user access
       kubectl delete rolebinding <user-binding> -n <namespace>
       ```

    5. **Notify stakeholders**
       - Security team
       - Engineering team lead
       - Compliance officer (if applicable)

    ### Investigation
    1. Review audit logs for full timeline
    2. Check if credentials were compromised
    3. Identify any other affected resources
    4. Determine if malicious or accidental

    ### Remediation
    1. Restore proper HPA configuration
    2. Implement additional controls
    3. Update RBAC if needed
    4. Rotate credentials if compromised

    ### Post-Incident
    1. Document findings
    2. Update runbook
    3. Conduct post-mortem
    4. Implement preventive measures

    ---

    ## Scenario 2: Resource Exhaustion Attack

    ### Detection
    - Alert: "ClusterCPUPressure"
    - Symptoms: Pods pending, slow scaling

    ### Immediate Actions
    1. **Check cluster capacity**
       ```bash
       kubectl top nodes
       kubectl get pods -A | grep Pending | wc -l
       ```

    2. **Identify runaway autoscaler**
       ```bash
       kubectl get hpa -A --sort-by=.status.currentReplicas
       ```

    3. **Emergency scale-down**
       ```bash
       kubectl patch hpa <name> -n <namespace> -p '{"spec":{"maxReplicas":5}}'
       ```

    4. **Add nodes if needed**
       ```bash
       # AWS
       aws autoscaling set-desired-capacity \
         --auto-scaling-group-name <asg> \
         --desired-capacity <N>
       ```

    ### Recovery
    1. Analyze what triggered excessive scaling
    2. Review HPA configuration
    3. Implement rate limits
    4. Update monitoring

    ---

    ## Scenario 3: Compliance Violation

    ### Detection
    - Alert: "PCI workload scaled to non-compliant zone"
    - Source: Policy violation

    ### Immediate Actions
    1. **Identify affected workloads**
       ```bash
       kubectl get pods -n pci-workloads -o wide
       ```

    2. **Check node compliance status**
       ```bash
       kubectl get nodes -l compliance-zone!=certified
       ```

    3. **Drain non-compliant nodes**
       ```bash
       kubectl drain <node> --ignore-daemonsets --delete-emptydir-data
       ```

    4. **Force reschedule to compliant nodes**
       ```bash
       kubectl delete pods -n pci-workloads --field-selector spec.nodeName=<non-compliant-node>
       ```

    ### Reporting
    1. Document violation for audit
    2. Report to compliance officer
    3. Update compliance report
    4. Implement additional safeguards
```

## Key Takeaways

### Security Principles

1. **Principle of Least Privilege**: Users and applications get minimum required permissions
2. **Defense in Depth**: Multiple layers of security controls
3. **Zero Trust**: Verify everything, trust nothing
4. **Audit Everything**: Comprehensive logging for forensics
5. **Automate Compliance**: Policy-as-code with Gatekeeper

### Compliance Requirements

| Framework | Key Requirements | Implementation |
|-----------|------------------|----------------|
| **PCI-DSS** | High availability, audit logging, access control | Min 2 replicas, comprehensive auditing, strict RBAC |
| **HIPAA** | Encryption, data residency, audit trails | Encrypted volumes, zone constraints, audit shipping |
| **SOC 2** | Change management, monitoring, access reviews | GitOps, alerts, quarterly RBAC audits |
| **GDPR** | Data locality, right to deletion | Region constraints, PV cleanup automation |

### Best Practices

âœ… **RBAC**: Implement least privilege for all users and ServiceAccounts
âœ… **Policies**: Enforce limits with Gatekeeper/OPA
âœ… **Multi-Tenancy**: Isolate tenants with namespaces, network policies, and node pools
âœ… **Audit**: Ship logs to external system with 7-year retention
âœ… **Compliance**: Implement framework-specific controls
âœ… **Monitoring**: Alert on unauthorized changes and violations
âœ… **Testing**: Regular security audits and penetration testing

## Related Topics

### Autoscaling Series (Complete)
- **[Part 1: HPA Fundamentals](./kubernetes-autoscaling-complete-guide-part1-horizontal-pod-autoscaler.md)**
- **[Part 2: Cluster Autoscaling](./kubernetes-autoscaling-complete-guide-part2-cluster-autoscaling.md)**
- **[Part 3: Hands-On Demo](./kubernetes-autoscaling-complete-guide-part3-hands-on-hpa-demo.md)**
- **[Part 4: Monitoring & Alerting](./kubernetes-autoscaling-complete-guide-part4-monitoring-alerting.md)**
- **[Part 5: VPA & Resource Optimization](./kubernetes-autoscaling-complete-guide-part5-vpa-resource-optimization.md)**
- **[Part 6: Advanced Patterns](./kubernetes-autoscaling-complete-guide-part6-advanced-patterns.md)**
- **[Part 7: Troubleshooting & War Stories](./kubernetes-autoscaling-complete-guide-part7-troubleshooting-war-stories.md)**

## Conclusion

Security and governance are not afterthoughtsâ€”they're foundational to production Kubernetes autoscaling. This guide established:

1. **RBAC Framework**: Hierarchical access control for all stakeholders
2. **Policy Enforcement**: Automated guardrails with Gatekeeper/OPA
3. **Multi-Tenancy**: Secure isolation patterns for shared clusters
4. **Compliance**: PCI-DSS, HIPAA, SOC 2, and GDPR requirements
5. **Audit**: Comprehensive logging and forensics capabilities
6. **Incident Response**: Runbooks for security incidents

### Implementation Roadmap

**Week 1: Foundation**
- Implement RBAC roles and bindings
- Deploy Gatekeeper
- Configure basic policies

**Week 2: Policies**
- Enforce HPA/VPA limits
- Implement namespace quotas
- Configure network policies

**Week 3: Audit & Compliance**
- Enable comprehensive audit logging
- Configure log shipping
- Implement framework-specific controls

**Week 4: Testing & Hardening**
- Security scanning
- Penetration testing
- Incident response drills

### The Complete Journey

You've now completed the **8-part Kubernetes Autoscaling Complete Guide**:

1. âœ… **Theory**: HPA fundamentals and approaches
2. âœ… **Infrastructure**: Cluster autoscaling strategies
3. âœ… **Practice**: Hands-on implementation
4. âœ… **Observability**: Monitoring and alerting
5. âœ… **Optimization**: VPA and cost reduction
6. âœ… **Advanced**: Complex patterns and architectures
7. âœ… **Operations**: Troubleshooting production issues
8. âœ… **Governance**: Security and compliance

From basic HPA to enterprise-grade security, you now have the complete toolkit for production Kubernetes autoscaling.

**Stay secure!** ğŸ”’ğŸ›¡ï¸

---

*This concludes the Kubernetes Autoscaling Complete Guide series. Thank you for reading!*
